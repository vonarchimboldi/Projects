The paper, "Distilling the Knowledge in a Neural Network" (Hinton et al. 2015) develops a methodology to transfer knowledge between classification learning models. Although knowledge transfer between models can be desirable in different practical scenarios, the paper highlights as one of the objectives achieving simpler models that obtain similar performance as cumbersome models or ensamble of specialist models without their computational overhead.
The strategy to generate this simpler model implies:
(1) Train cumbersome models or ensambles of specialists models that maximize predictivity without taking into account computation cost constraints.
(2) Reutilize knowledge generated by the cumbersome models to train simpler models. Specifically, the knowledge reutilized takes the form of the individual class probabilities.
Generally the cumbersome model would have been trained with the objective of maximizing the average log probability of the correct class. As a side-effect of the patterns identified, it will also assign probabilities for all the other classes. For example, in a network trained to recognized MNIST written numbers, even if a cumbersome model predicts correctly that a written number is a 7, if the specific number was similar to a 3 the model will assign probability to the 3 class recognizing a pattern between the inputs and the class.

The distillation techique allows simpler models to utilize this additional information/knowledge generated by the cumbersome model to overperform other models with the same complexity trained using only the class targets.
